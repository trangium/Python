{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8508, 0.5553, 0.3821, 0.0810, 0.1335],\n",
      "        [0.7168, 0.6579, 0.9628, 0.1862, 0.4992],\n",
      "        [0.2169, 0.9496, 0.6143, 0.6141, 0.6076]])\n",
      "tensor([0.7168, 0.6579, 0.9628, 0.1862, 0.4992])\n",
      "tensor([1.8925e-01, 1.2326e-01, 8.2734e-01, 2.2409e-04, 3.0993e-02])\n"
     ]
    }
   ],
   "source": [
    "orz = torch.rand(3, 5)\n",
    "print(orz)\n",
    "print(orz[1])\n",
    "print(orz[1] ** 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch                     # for all things PyTorch\n",
    "import torch.nn as nn            # for torch.nn.Module, the parent object for PyTorch models\n",
    "import torch.nn.functional as F  # for the activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        # 1 input image channel (black & white), 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 5*5 from image dimension\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n",
      "\n",
      "Image batch shape:\n",
      "torch.Size([1, 1, 32, 32])\n",
      "\n",
      "Raw output:\n",
      "tensor([[ 0.0624, -0.0094,  0.0970,  0.0562, -0.0437, -0.0166, -0.0271, -0.0637,\n",
      "          0.0494,  0.0171]], grad_fn=<AddmmBackward0>)\n",
      "torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "net = LeNet()\n",
    "print(net)                         # what does the object tell us about itself?\n",
    "\n",
    "input = torch.rand(1, 1, 32, 32)   # stand-in for a 32x32 black & white image\n",
    "print('\\nImage batch shape:')\n",
    "print(input.shape)\n",
    "\n",
    "output = net(input)                # we don't call forward() directly\n",
    "print('\\nRaw output:')\n",
    "print(output)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1c2ff8e9750>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1729)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.1257, -0.0057, -1.3975,  1.4364, -0.1068, -0.8413,  0.7753, -0.5475,\n",
      "          1.2442, -0.1366]])\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_h = torch.randn(1, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4636, -0.4589,  1.5013, -0.2707,  0.5898, -1.0872, -0.2940, -0.7889,\n",
      "         -0.1804, -0.5438, -0.1018,  0.3261,  1.9147, -0.5373, -1.4944,  0.5240,\n",
      "         -1.4364, -0.5617, -1.1327,  0.0796]])\n"
     ]
    }
   ],
   "source": [
    "print(prev_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_h = torch.randn(20, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3449,  0.6962, -0.2622, -0.7324, -1.5412,  1.7046, -0.3845,  0.3328,\n",
      "         -0.6018, -1.0326,  0.9224,  0.6983,  0.4298,  0.7367, -0.6765, -1.6013,\n",
      "         -0.4333,  0.0672,  1.1396,  0.8319],\n",
      "        [-2.8290,  0.7990, -0.0259, -0.1426,  0.3132, -0.0052,  0.2564,  0.5316,\n",
      "          0.9480, -0.8093,  1.6049, -0.3759,  1.6718, -1.7147,  0.7230, -1.0887,\n",
      "         -1.8443,  0.9148, -1.3802, -2.2965],\n",
      "        [ 0.1192, -0.1975, -0.0881, -1.5910,  1.1006,  0.4278,  0.3104,  0.7605,\n",
      "         -1.2862,  2.3782,  0.1931, -0.7604, -0.3729, -0.3506,  0.6184, -0.8416,\n",
      "         -0.5616,  0.0978, -1.1480,  1.0007],\n",
      "        [-1.1884, -0.7783, -0.2389,  0.2970, -1.5321, -0.6058, -1.2782,  0.4728,\n",
      "         -0.4541,  0.4006,  0.3103, -0.8570, -1.7144,  1.2718,  0.5791,  1.0235,\n",
      "         -1.2904, -1.0635,  1.2939,  0.6420],\n",
      "        [-0.6757,  0.1376, -0.4425, -0.0483, -1.5344,  0.4059,  0.6875,  0.8909,\n",
      "         -0.5431,  1.4094, -1.8051, -0.7836,  0.5342, -0.5598,  0.8304, -0.4330,\n",
      "          0.3454, -0.0429,  1.2696, -0.0210],\n",
      "        [ 2.0489, -0.0093,  0.6655,  0.9602,  0.5688, -0.6245, -0.6425,  0.7229,\n",
      "         -0.8575, -1.1869, -0.4189, -0.7533, -0.4253, -0.0375, -0.4577, -0.7342,\n",
      "         -2.7676, -1.7156,  0.6794,  0.2896],\n",
      "        [ 1.7408,  0.3355,  1.2305, -2.0843, -0.7219, -0.0297,  0.1692,  0.6099,\n",
      "          0.1490, -0.9612,  0.3432,  0.9819,  1.4268, -1.7648, -0.1755, -0.9692,\n",
      "         -0.5314, -1.4093,  0.5555,  0.2686],\n",
      "        [ 0.2843, -0.0822, -0.9507, -1.6051,  0.4736, -0.4712, -1.5579,  0.4962,\n",
      "         -0.7555, -1.3688, -0.3877, -0.7936, -0.0720, -0.3293,  1.1433, -0.0903,\n",
      "          1.4168, -0.4885, -0.4915, -1.4630],\n",
      "        [-0.5020,  0.9245,  0.5732, -0.2248,  0.4204,  0.8178,  0.7636, -0.8672,\n",
      "          0.6552, -0.1430,  0.7466,  0.8826,  0.4604,  0.0115, -0.5554, -1.3413,\n",
      "         -1.2512,  4.4448, -0.3885,  0.4990],\n",
      "        [ 1.0166,  1.4747, -0.1150,  0.7332, -0.9461, -0.4791,  1.0731, -0.6877,\n",
      "          0.6783,  0.2265, -0.7555,  0.7533,  0.7068,  2.1148, -1.1349, -1.2006,\n",
      "         -1.8804, -0.6098,  0.3272,  0.2144],\n",
      "        [-0.0680,  1.6767, -1.0277, -1.3693, -0.3001, -1.6340,  0.0866, -0.2758,\n",
      "         -0.4217,  0.2053, -0.3248, -1.1405,  1.9121,  1.4002, -0.1931, -0.4796,\n",
      "         -0.7069, -1.8824,  0.1761,  1.0782],\n",
      "        [-1.0556, -0.5558, -1.9200, -1.8202, -0.2890, -0.6617, -0.5416,  1.0281,\n",
      "          0.4687, -0.2058, -0.6222,  0.5398, -1.8285,  0.2280, -0.3444,  1.3270,\n",
      "          1.2390, -2.2408, -0.7300,  0.8479],\n",
      "        [-1.3873, -1.1191,  0.1456,  2.1266, -0.5125, -2.2747, -0.8282,  1.1131,\n",
      "          1.6882, -0.7695,  0.3600, -0.2626, -0.8321,  1.6281,  1.0251,  0.6948,\n",
      "          0.5590,  2.0323, -0.9884, -0.4131],\n",
      "        [ 0.1516,  0.2414,  0.2563, -0.0531, -0.6311,  0.5453, -0.0252,  0.5223,\n",
      "          1.3654,  0.1383, -1.8493,  0.6919,  0.8172, -0.4277,  0.2178, -0.4573,\n",
      "         -2.4521,  0.0985, -0.9667, -1.3629],\n",
      "        [ 1.1884,  0.2557, -0.0751,  0.8869,  0.2049, -0.2453,  0.6019, -0.3624,\n",
      "         -0.1375,  0.6288, -2.5166,  1.7099, -1.0722, -0.3414,  0.5866,  0.4723,\n",
      "          0.2638,  0.3904,  0.6896, -1.1161],\n",
      "        [-0.3760, -1.4252,  0.3213, -0.3162,  1.5141, -1.0713, -0.6994,  1.4845,\n",
      "          0.3387, -0.0530,  0.1822, -0.5610,  0.0574,  0.6708, -0.2466, -0.6220,\n",
      "         -0.5752, -0.7230, -0.0475,  0.8267],\n",
      "        [ 0.5903,  0.1811, -0.5116, -0.4806,  0.7797,  0.0516,  0.2447, -0.5064,\n",
      "         -0.2097,  1.6876, -0.8580, -0.0365, -0.7932, -0.2898, -2.1482,  1.5964,\n",
      "         -0.5516, -0.2969, -0.6816,  0.0211],\n",
      "        [ 0.6836, -0.2058,  1.5907,  2.3072, -0.7813,  0.7156, -1.4366, -0.0797,\n",
      "         -0.7189,  1.1100, -0.2009, -0.7871,  1.0345,  0.3992,  0.8793, -0.2966,\n",
      "          2.3251, -1.6103,  0.0169, -0.4052],\n",
      "        [ 1.0109,  0.9745,  0.7489,  2.2523, -1.9764, -0.6632, -1.8817,  0.8132,\n",
      "         -0.8831,  1.6232, -0.5327,  0.9873, -0.5029,  1.9732, -1.4635,  1.3772,\n",
      "          1.5655, -0.1288, -0.1347, -0.0062],\n",
      "        [ 0.1992, -1.0161, -1.1951,  1.1465,  0.9474,  0.2958, -0.2656, -2.4934,\n",
      "         -0.4964,  2.3714, -0.7281, -0.4684, -1.8081, -0.9626,  1.8931, -0.2044,\n",
      "          0.1573,  1.2359, -0.5600, -0.8851]])\n"
     ]
    }
   ],
   "source": [
    "print(W_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_x = torch.randn(20, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.3934e+00, -3.4110e-01, -1.4382e+00,  1.2331e+00, -9.7630e-02,\n",
      "         -2.8418e-01, -2.8721e-01,  1.1003e+00,  1.0374e+00,  2.1710e-01],\n",
      "        [-4.6288e-01, -2.4551e-01, -7.5455e-01, -1.2574e+00,  2.2531e-01,\n",
      "          3.2977e-01, -2.3607e+00,  4.5326e-01,  1.1335e+00,  1.3151e-01],\n",
      "        [ 1.0117e+00,  5.1343e-01,  1.5889e+00, -2.0537e-01,  5.2540e-01,\n",
      "          1.1568e+00,  7.7021e-01,  5.0064e-02,  1.4679e+00,  2.6712e-01],\n",
      "        [ 8.2550e-01, -1.5017e-01,  1.8853e-01,  6.0882e-01, -2.6476e-01,\n",
      "         -6.7771e-01,  1.5736e+00,  4.5240e-01,  3.7509e-01,  2.8655e-01],\n",
      "        [ 4.8541e-01, -1.3093e+00, -1.0888e+00,  2.7921e+00,  9.8802e-01,\n",
      "         -5.3969e-01,  7.1255e-01, -1.3497e+00,  5.3468e-01, -3.1235e-01],\n",
      "        [-5.1749e-04, -3.5624e-01, -6.3068e-01, -1.0360e+00,  5.3676e-01,\n",
      "          4.2574e-01,  7.9390e-01,  3.3186e-01,  1.4483e+00,  4.4602e-01],\n",
      "        [-1.5044e-01, -1.2368e+00, -1.1558e+00, -3.0528e-01,  9.8272e-01,\n",
      "          4.0460e-01, -6.7378e-01,  1.0127e+00, -9.6466e-01,  2.9710e+00],\n",
      "        [-9.6453e-01,  1.2772e+00, -4.9290e-01, -1.3502e+00,  1.1819e+00,\n",
      "          9.4652e-01, -1.7549e+00,  2.8263e-01,  1.0035e-01,  3.7563e-01],\n",
      "        [ 9.9993e-01,  1.8383e+00,  6.5901e-01,  4.6944e-01, -2.2178e-01,\n",
      "          5.6424e-01, -1.7379e+00,  8.7898e-01,  1.6151e-01, -3.6919e-01],\n",
      "        [-1.3640e-01, -3.2673e-01, -9.5607e-01,  2.9817e-01,  3.3482e-01,\n",
      "         -2.2482e-01,  3.2339e-01,  4.1773e-01,  4.5542e-01, -1.0908e+00],\n",
      "        [ 9.6326e-01,  4.9397e-01,  6.9445e-01,  1.3751e+00, -9.0191e-01,\n",
      "          2.9983e-01, -1.2196e+00, -7.3578e-01, -3.2342e-02,  2.5715e-01],\n",
      "        [-1.9029e-01, -1.1412e+00,  9.7082e-02,  1.6706e-02, -1.7687e-01,\n",
      "         -3.9943e-01, -4.5533e-01, -1.2893e+00,  3.5656e-01,  2.9275e-01],\n",
      "        [-2.1180e+00,  6.0888e-03,  3.3050e-01, -1.0628e+00, -1.5699e+00,\n",
      "          4.1407e-02,  7.6991e-01,  2.1716e+00, -2.1295e+00,  1.1273e+00],\n",
      "        [-2.2231e+00, -5.6646e-01, -2.3486e-01, -1.3779e+00,  2.2656e+00,\n",
      "         -3.0629e+00,  3.2540e-01,  1.6766e+00, -8.5637e-01,  1.1947e+00],\n",
      "        [ 1.2559e+00,  6.7716e-01,  1.9841e-01, -2.2359e+00,  1.2400e+00,\n",
      "         -6.2064e-01, -7.6094e-01, -2.2356e-01, -1.9728e+00,  5.1320e-01],\n",
      "        [ 3.9826e-01,  1.5176e+00, -3.2844e-01, -1.3225e-01, -1.8554e+00,\n",
      "          1.0911e+00, -2.3819e-01,  1.3468e+00, -9.9092e-01,  1.2317e+00],\n",
      "        [ 1.1131e+00, -7.0590e-01, -9.3540e-02, -4.6404e-01,  7.1330e-02,\n",
      "          1.3388e+00,  3.8768e-01,  6.5458e-01,  1.1544e+00, -2.3979e-02],\n",
      "        [ 8.8168e-01,  1.6131e-01,  9.0572e-01,  3.7429e-01, -4.2287e-01,\n",
      "         -1.5606e+00,  1.9563e-01, -6.4726e-01, -4.5917e-01,  1.6655e+00],\n",
      "        [-2.6293e-02,  1.3715e+00,  5.4329e-01, -1.3118e+00,  1.0395e+00,\n",
      "         -8.6379e-01,  6.1256e-01, -4.7551e-01,  4.0735e-01, -4.0393e-01],\n",
      "        [ 5.5541e-01,  7.7360e-01,  1.3659e+00, -2.6399e-01,  1.0402e+00,\n",
      "          6.8461e-01, -5.1340e-01,  1.6990e+00, -1.3142e-01, -1.3397e+00]])\n"
     ]
    }
   ],
   "source": [
    "print(W_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "i2h = torch.mm(W_x, x.t())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2h = torch.mm(W_h, prev_h.t())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_h = i2h+h2h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_h = next_h.tanh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 1])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h2h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 1])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i2h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = next_h.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-2.5845)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\vince\\Python\\torch.ipynb Cell 23\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/vince/Python/torch.ipynb#X33sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    494\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m     tensors,\n\u001b[0;32m    253\u001b[0m     grad_tensors_,\n\u001b[0;32m    254\u001b[0m     retain_graph,\n\u001b[0;32m    255\u001b[0m     create_graph,\n\u001b[0;32m    256\u001b[0m     inputs,\n\u001b[0;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    259\u001b[0m )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'torch.nn.functional' from 'C:\\\\Users\\\\vince\\\\AppData\\\\Roaming\\\\Python\\\\Python311\\\\site-packages\\\\torch\\\\nn\\\\functional.py'>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        # 1 input image channel (black & white), 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 5*5 from image dimension\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = LeNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.conv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = torch.randn(8, 4, 3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-2.7939e-01,  1.1210e+00, -4.9333e-01],\n",
       "          [-1.3254e+00,  7.8855e-01, -2.0221e+00],\n",
       "          [ 5.8965e-01, -1.8399e+00, -4.6922e-01]],\n",
       "\n",
       "         [[ 5.9846e-02,  1.2744e+00,  1.3943e+00],\n",
       "          [-4.2417e-01,  3.4659e-01,  2.1147e-01],\n",
       "          [-7.1755e-01, -1.1315e+00,  2.1533e-01]],\n",
       "\n",
       "         [[-8.3253e-01, -3.9046e-01, -1.8886e-01],\n",
       "          [-5.8440e-01, -1.8890e+00, -1.5860e-01],\n",
       "          [ 9.3111e-01,  1.8836e+00, -9.5399e-01]],\n",
       "\n",
       "         [[ 7.5382e-01,  9.4419e-01,  1.1362e+00],\n",
       "          [ 4.6820e-01,  9.5190e-01, -5.9579e-01],\n",
       "          [ 4.1594e-01, -1.3759e+00,  1.0790e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 4.4616e-01, -1.2363e+00,  1.7025e-01],\n",
       "          [-7.8679e-01, -1.6657e+00,  3.8348e-01],\n",
       "          [-3.2962e-01, -4.6644e-01,  5.1478e-01]],\n",
       "\n",
       "         [[ 7.8638e-01, -7.5488e-02, -7.0654e-01],\n",
       "          [-1.8872e+00, -3.5692e-01, -1.4451e+00],\n",
       "          [ 8.5055e-01, -1.9754e+00, -7.2608e-01]],\n",
       "\n",
       "         [[ 2.6702e+00, -1.2743e+00,  2.2901e-01],\n",
       "          [ 5.9152e-01,  2.9941e-01, -2.0008e+00],\n",
       "          [ 4.6026e-01,  6.2640e-01,  4.6130e-01]],\n",
       "\n",
       "         [[-6.1600e-03,  1.2321e+00, -1.2112e+00],\n",
       "          [-6.9383e-01, -1.7298e+00, -9.3346e-01],\n",
       "          [-1.5526e+00, -1.1815e+00, -8.0010e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 1.5985e-01,  6.2661e-01, -1.2881e+00],\n",
       "          [-6.6995e-01,  6.3542e-01, -1.3485e+00],\n",
       "          [ 1.0731e-01, -1.6384e+00,  1.3921e+00]],\n",
       "\n",
       "         [[ 2.8310e-01,  2.8294e-01, -2.2865e+00],\n",
       "          [ 5.3365e-01,  8.2007e-02,  1.2485e+00],\n",
       "          [ 3.4859e-01,  1.2297e+00, -3.9081e-01]],\n",
       "\n",
       "         [[ 6.6713e-01,  9.9863e-01, -5.7122e-01],\n",
       "          [ 1.5467e+00, -1.7773e+00,  2.2469e-01],\n",
       "          [-6.4711e-01, -7.9374e-01, -7.4452e-02]],\n",
       "\n",
       "         [[ 2.8695e+00,  9.0699e-01, -8.3735e-01],\n",
       "          [ 2.9974e-01,  1.0351e+00,  2.0310e-01],\n",
       "          [-1.0557e+00, -2.1186e+00, -2.9208e-01]]],\n",
       "\n",
       "\n",
       "        [[[-1.0375e-01,  2.4771e+00,  5.8470e-01],\n",
       "          [-1.7494e+00,  1.2297e+00, -1.1235e+00],\n",
       "          [-1.6914e+00, -5.5325e-01, -1.4502e+00]],\n",
       "\n",
       "         [[ 1.5271e+00,  4.1059e-01, -2.0173e-01],\n",
       "          [ 7.9234e-01,  1.7424e+00, -1.4089e+00],\n",
       "          [-1.5988e+00, -1.2811e+00, -3.9503e-01]],\n",
       "\n",
       "         [[ 2.0122e-01, -1.4637e+00, -6.1071e-01],\n",
       "          [-2.0163e-01, -1.2409e+00, -1.5517e+00],\n",
       "          [-5.8898e-01,  2.3345e-01,  3.7555e-01]],\n",
       "\n",
       "         [[ 1.8525e-01, -8.3483e-01, -1.1400e+00],\n",
       "          [-2.9532e-01,  5.7144e-01,  3.7404e-01],\n",
       "          [ 2.2683e+00,  5.3693e-01,  1.4029e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 5.6685e-02, -8.9778e-01, -3.7768e-01],\n",
       "          [ 1.8676e-01, -1.2884e+00, -6.0387e-01],\n",
       "          [ 6.0986e-01, -1.9682e-01, -3.5821e-01]],\n",
       "\n",
       "         [[-7.1517e-01,  2.2560e-02, -7.6363e-01],\n",
       "          [ 9.7510e-01, -9.4355e-01,  1.2931e+00],\n",
       "          [ 6.7178e-01,  8.0539e-01,  6.0258e-01]],\n",
       "\n",
       "         [[-9.7377e-02, -1.4145e+00, -6.1858e-01],\n",
       "          [-7.4855e-01,  1.0178e+00, -3.2407e-01],\n",
       "          [ 2.0872e-02,  6.6061e-01, -8.0292e-01]],\n",
       "\n",
       "         [[ 8.2097e-01,  5.5847e-01, -3.9914e-01],\n",
       "          [ 1.2841e+00,  1.6038e+00,  7.6573e-01],\n",
       "          [-1.2456e+00,  4.8323e-01,  1.0627e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 4.6260e-02, -6.7831e-01, -2.2809e-01],\n",
       "          [-9.3411e-01,  3.8804e-01, -4.6913e-01],\n",
       "          [-2.6785e-01,  6.7616e-01, -3.7317e-01]],\n",
       "\n",
       "         [[ 2.0388e+00, -1.1449e+00, -1.0900e+00],\n",
       "          [-7.6694e-01,  5.9065e-01, -3.3789e-01],\n",
       "          [-1.9749e-01,  2.8946e-01,  7.8859e-01]],\n",
       "\n",
       "         [[-3.2496e-01,  1.2241e+00, -9.0142e-01],\n",
       "          [-8.6043e-01, -3.2868e-01,  5.4625e-01],\n",
       "          [ 1.4216e+00, -1.3505e+00,  2.0652e-02]],\n",
       "\n",
       "         [[-2.9850e-01,  1.6391e+00, -1.3982e+00],\n",
       "          [-1.0063e-01,  1.0529e+00,  2.6021e-01],\n",
       "          [ 5.9329e-02,  1.2917e+00, -3.0128e-01]]],\n",
       "\n",
       "\n",
       "        [[[-8.3056e-01, -1.1720e+00,  1.7286e+00],\n",
       "          [ 4.8033e-01,  3.7816e-02,  1.2781e+00],\n",
       "          [-1.9894e+00,  1.2549e-01,  6.6159e-01]],\n",
       "\n",
       "         [[ 1.2437e+00, -9.4854e-01,  4.0835e-01],\n",
       "          [-2.0169e-01,  1.3756e+00,  1.1299e+00],\n",
       "          [ 1.7185e-01,  7.5794e-01,  5.7606e-01]],\n",
       "\n",
       "         [[ 1.7291e-01,  9.9564e-01, -1.7158e+00],\n",
       "          [ 1.0537e+00, -3.8232e-01, -3.3659e-01],\n",
       "          [ 1.1004e+00,  1.1893e+00,  3.4698e-01]],\n",
       "\n",
       "         [[ 1.4474e+00,  5.1308e-01,  1.4640e-01],\n",
       "          [ 5.9629e-01, -2.7204e-03,  9.7030e-01],\n",
       "          [-1.2483e-01, -2.2442e+00, -1.2417e+00]]],\n",
       "\n",
       "\n",
       "        [[[-1.0042e+00, -3.2647e-02,  8.5183e-01],\n",
       "          [-6.6108e-01,  1.3671e-01, -5.4238e-01],\n",
       "          [ 7.6147e-02, -1.6588e+00,  1.1745e+00]],\n",
       "\n",
       "         [[-2.6724e+00,  9.7400e-02, -1.1475e+00],\n",
       "          [-1.3518e+00, -9.2908e-01,  3.9755e-01],\n",
       "          [ 1.4113e+00,  3.5030e-02,  5.1901e-01]],\n",
       "\n",
       "         [[ 6.9326e-01,  1.2497e+00,  6.0017e-01],\n",
       "          [-1.0338e+00,  8.1969e-01, -8.7071e-01],\n",
       "          [-7.2418e-02,  5.0984e-01,  1.2437e+00]],\n",
       "\n",
       "         [[ 8.3514e-01, -1.4214e-02,  5.7157e-01],\n",
       "          [-6.0425e-01,  3.7600e-01,  1.3785e+00],\n",
       "          [-7.2059e-01,  9.7914e-01, -4.0501e-01]]]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.randn(1, 4, 5, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[  6.1111,  -4.2767,  -5.7457,   0.5115,   4.3099],\n",
       "          [ -1.0470,   3.5101,   2.2150,   3.2050, -12.0533],\n",
       "          [ -1.0352,  19.6281,  -1.7912,   0.0626,  -0.6121],\n",
       "          [  8.9096,   5.1375,   5.6179,  -4.1295,   4.5121],\n",
       "          [  1.7254,  -0.6964,  10.3680,   5.2422,  -1.7886]],\n",
       "\n",
       "         [[ -5.1343, -10.4448,  -1.5885,  -1.4133,   3.8210],\n",
       "          [ -2.3841, -16.5861,  -2.4003,  -3.2805,  -1.8252],\n",
       "          [ -3.3113,   8.3457,  -0.2644,   5.5710,  -6.0940],\n",
       "          [  5.8794,   3.7659,  -5.9518,  -2.6017,   3.7631],\n",
       "          [  8.2765,   3.1774,   0.3278,  -6.9519,   5.7087]],\n",
       "\n",
       "         [[ -0.8971,  -8.5262,   1.8626,   5.7733,  -0.0356],\n",
       "          [ -1.9430,   5.0611,  -2.8845,   6.9445, -11.7910],\n",
       "          [ -3.9630,   4.5160,   2.9332,   0.9558,   6.4843],\n",
       "          [ -0.2009,   2.8854,  -0.2294,  -2.3820,   8.0870],\n",
       "          [  4.2749,  -2.3008,   6.8000,   1.6439,  -3.3029]],\n",
       "\n",
       "         [[  1.6746,   3.8615,  -4.0178,   5.3526,  -8.1931],\n",
       "          [ -4.2417,   4.7487,   1.0524,   6.0807,   0.0752],\n",
       "          [ -8.6667,  10.9252,   5.4042,   8.6014,  -5.3852],\n",
       "          [  4.3690,   5.4501,   9.0462,   2.7451,  13.7539],\n",
       "          [  3.1399,  -6.0868,   0.7993,   0.1435,   4.3770]],\n",
       "\n",
       "         [[  2.1639,   5.6133,   4.2628,  -5.7324,   6.5699],\n",
       "          [ 10.6467,  -1.6587,   4.4193,  -2.3473,  -3.2067],\n",
       "          [  1.4009,  -1.7556,  -0.5116,  -1.6353,  -4.3251],\n",
       "          [ -3.5861,   8.9458,  -2.3220,   3.8450,  -5.1108],\n",
       "          [  4.7744,   2.6991,   7.7833,   5.0811,   3.6933]],\n",
       "\n",
       "         [[  2.0866,   7.8236,  -0.2843,   3.2233,  -7.7342],\n",
       "          [ -2.6973,   7.8374,  -3.6162,  -1.3709,   2.3916],\n",
       "          [  2.0445,  -8.0798,  10.3806,  -3.4543,   6.8514],\n",
       "          [  4.7825,  -2.8794,   8.5063,  -0.3605,  -4.1194],\n",
       "          [ -1.0845,   5.1167,   2.6120,   3.7428,  -2.0226]],\n",
       "\n",
       "         [[ -3.4951,  -3.5498,   5.7234,   6.7678,   2.4070],\n",
       "          [ -4.3010,  11.3581,   6.8552,  -5.8324,  -2.2137],\n",
       "          [  1.9700,  -3.7850,   8.1203,   2.9084,   3.3493],\n",
       "          [ -1.4995,  -6.3201,  -5.8797,   2.6315,   9.0270],\n",
       "          [-12.5472,   4.8032,  -4.3943,   2.1159,  -2.4300]],\n",
       "\n",
       "         [[  2.2479,   3.7176,   6.2007,  -0.7539,   1.4749],\n",
       "          [  7.9875,  -5.1881,  -3.6888,  -8.4188,  -7.5792],\n",
       "          [ -0.1995,   3.8665,  -3.1428,  -1.1068,   0.4643],\n",
       "          [ -5.3914,   8.8983,  -2.7225,  12.5202,  -0.9906],\n",
       "          [  0.1477,   5.6325,  11.4461,  -0.1229,  -5.0470]]]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.conv2d(inputs, filters, padding=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = torch.randn(2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0127,  0.7546],\n",
       "        [-0.3922, -1.1310]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.randn(4, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6320,  1.1936, -0.4661],\n",
       "        [ 0.0559, -1.1309,  1.3115],\n",
       "        [ 1.0748, -2.1432,  1.7882],\n",
       "        [ 1.1084, -0.9052, -0.2836]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [4, 3]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\vince\\Python\\torch.ipynb Cell 37\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/vince/Python/torch.ipynb#X53sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m F\u001b[39m.\u001b[39;49mconv2d(inputs, filters)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [4, 3]"
     ]
    }
   ],
   "source": [
    "F.conv2d(inputs, filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = nn.Conv2d(4, 1000, 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 30, 54])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp(torch.randn(4, 29+14, 53+14)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Image batch shape:\n",
      "torch.Size([1, 1, 32, 32])\n",
      "\n",
      "Raw output:\n",
      "tensor([[-0.0103,  0.0929,  0.1042,  0.0222, -0.0098, -0.0363,  0.0322, -0.0232,\n",
      "         -0.0596, -0.1184]], grad_fn=<AddmmBackward0>)\n",
      "torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "input = torch.rand(1, 1, 32, 32)   # stand-in for a 32x32 black & white image\n",
    "print('\\nImage batch shape:')\n",
    "print(input.shape)\n",
    "\n",
    "output = net(input)                # we don't call forward() directly\n",
    "print('\\nRaw output:')\n",
    "print(output)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = net.forward(torch.rand(1, 1, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsf = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0, 0, 0), (0.5, 0.5, 0.5))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = torchvision.datasets.CIFAR10(root=\"./data\", train=True, download=False, transform=tsf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset CIFAR10\n",
       "    Number of datapoints: 50000\n",
       "    Root location: ./data\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               ToTensor()\n",
       "               Normalize(mean=(0, 0, 0), std=(0.5, 0.5, 0.5))\n",
       "           )"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + 0.5\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size = 4, shuffle = True, num_workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(dataiter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader._MultiProcessingDataLoaderIter at 0x1c29010b090>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'matplotlib' has no attribute 'imshow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\vince\\Python\\torch.ipynb Cell 56\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/vince/Python/torch.ipynb#Y111sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m imshow(torchvision\u001b[39m.\u001b[39;49mutils\u001b[39m.\u001b[39;49mmake_grid(images))\n",
      "\u001b[1;32mc:\\Users\\vince\\Python\\torch.ipynb Cell 56\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/vince/Python/torch.ipynb#Y111sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m img \u001b[39m=\u001b[39m img \u001b[39m/\u001b[39m \u001b[39m2\u001b[39m \u001b[39m+\u001b[39m \u001b[39m0.5\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/vince/Python/torch.ipynb#Y111sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m npimg \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/vince/Python/torch.ipynb#Y111sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m plt\u001b[39m.\u001b[39;49mimshow(np\u001b[39m.\u001b[39mtranspose(npimg, (\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m)))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\matplotlib\\_api\\__init__.py:217\u001b[0m, in \u001b[0;36mcaching_module_getattr.<locals>.__getattr__\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m props:\n\u001b[0;32m    216\u001b[0m     \u001b[39mreturn\u001b[39;00m props[name]\u001b[39m.\u001b[39m\u001b[39m__get__\u001b[39m(instance)\n\u001b[1;32m--> 217\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\n\u001b[0;32m    218\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmodule \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__module__\u001b[39m\u001b[39m!r}\u001b[39;00m\u001b[39m has no attribute \u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m!r}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'matplotlib' has no attribute 'imshow'"
     ]
    }
   ],
   "source": [
    "imshow(torchvision.utils.make_grid(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imshow(torchvision.utils.make_grid(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['horse', 'ship', 'ship', 'cat']"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[classes[labels[i]] for i in range(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transform' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\vince\\Python\\torch.ipynb Cell 62\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/vince/Python/torch.ipynb#Y120sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m trainset \u001b[39m=\u001b[39m torchvision\u001b[39m.\u001b[39mdatasets\u001b[39m.\u001b[39mCIFAR10(root\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m./data\u001b[39m\u001b[39m'\u001b[39m, train\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/vince/Python/torch.ipynb#Y120sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m                                         download\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, transform\u001b[39m=\u001b[39mtransform)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/vince/Python/torch.ipynb#Y120sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m trainloader \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataLoader(trainset, batch_size\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/vince/Python/torch.ipynb#Y120sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m                                           shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, num_workers\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/vince/Python/torch.ipynb#Y120sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m testset \u001b[39m=\u001b[39m torchvision\u001b[39m.\u001b[39mdatasets\u001b[39m.\u001b[39mCIFAR10(root\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m./data\u001b[39m\u001b[39m'\u001b[39m, train\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/vince/Python/torch.ipynb#Y120sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m                                        download\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, transform\u001b[39m=\u001b[39mtransform)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'transform' is not defined"
     ]
    }
   ],
   "source": [
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                         shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=tsf)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=tsf)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                         shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 2.303\n",
      "[1,  4000] loss: 2.303\n",
      "[1,  6000] loss: 2.304\n",
      "[1,  8000] loss: 2.303\n",
      "[1, 10000] loss: 2.303\n",
      "[1, 12000] loss: 2.304\n",
      "[2,  2000] loss: 2.303\n",
      "[2,  4000] loss: 2.304\n",
      "[2,  6000] loss: 2.303\n",
      "[2,  8000] loss: 2.302\n",
      "[2, 10000] loss: 2.304\n",
      "[2, 12000] loss: 2.304\n",
      "[3,  2000] loss: 2.304\n",
      "[3,  4000] loss: 2.303\n",
      "[3,  6000] loss: 2.304\n",
      "[3,  8000] loss: 2.304\n",
      "[3, 10000] loss: 2.303\n",
      "[3, 12000] loss: 2.303\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(3):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
       "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
       "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGD (\n",
       "Parameter Group 0\n",
       "    dampening: 0\n",
       "    differentiable: False\n",
       "    foreach: None\n",
       "    lr: 0.001\n",
       "    maximize: False\n",
       "    momentum: 0.9\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
       "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
       "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 2.220\n",
      "[1,  4000] loss: 1.971\n",
      "[1,  6000] loss: 1.830\n",
      "[1,  8000] loss: 1.731\n",
      "[1, 10000] loss: 1.624\n",
      "[1, 12000] loss: 1.600\n",
      "[2,  2000] loss: 1.519\n",
      "[2,  4000] loss: 1.517\n",
      "[2,  6000] loss: 1.482\n",
      "[2,  8000] loss: 1.484\n",
      "[2, 10000] loss: 1.475\n",
      "[2, 12000] loss: 1.426\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 50 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand = torch.rand(2, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "doubled = rand * (torch.ones(4) * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0291, 0.8526, 0.0462, 1.1287],\n",
       "        [1.3482, 1.2176, 1.4183, 0.3818]])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doubled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after 'if' statement on line 1 (537574416.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[142], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    print('We have a GPU!')\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block after 'if' statement on line 1\n"
     ]
    }
   ],
   "source": [
    "    if torch.cuda.is_available():\n",
    "    print('We have a GPU!')\n",
    "else:\n",
    "    print('Sorry, CPU only.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have a GPU!\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print('We have a GPU!')\n",
    "else:\n",
    "    print('Sorry, CPU only.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(5, 128),  # Input size 64, output size 128\n",
    "    nn.ReLU(),          # ReLU activation function\n",
    "    nn.Linear(128, 64),  # Input size 128, output size 64\n",
    "    nn.ReLU(),          # ReLU activation function\n",
    "    nn.Linear(64, 2)    # Input size 64, output size 10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = torch.tensor([0.5, -0.5, 1.1, 0.3, 1.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc(sample):\n",
    "    dist = (sample[2]-sample[0])**2 + (sample[3] - sample[1])**2\n",
    "    return torch.tensor([dist, 1 if dist < sample[4] else 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.MSELoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1181, 0.0073], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = loss_fn(model(sample), calc(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.7633, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1.])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1181, 0.0073], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for param in model.parameters():\n",
    "        param -= learning_rate * param.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8659, 0.7501], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = torch.tensor([0.5, -1.2, -0.3, 0.2, 0.7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.6000, 0.0000])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc(sample):\n",
    "    dist = ((sample[2]-sample[0])**2 + (sample[3] - sample[1])**2)**0.5\n",
    "    return torch.tensor([dist, 1 if dist < sample[4] else 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = torch.tensor([0.5, -1.2, -0.3, 0.2, 0.7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.6125, 0.0000])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = torch.tensor([0.5, -1.2, -0.3, 0.2, 1.4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = torch.tensor([0.5, -0.1, -0.3, 0.2, 1.4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8544, 1.0000])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = loss_fn(model(sample), calc(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2124, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6552, 0.5844], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for param in model.parameters():\n",
    "        param -= learning_rate * param.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7401, 0.6878], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.1622, -2.8167])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
